{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mtensorflow 1.7.1 has requirement numpy>=1.13.3, but you'll have numpy 1.12.1 which is incompatible.\u001b[0m\r\n",
      "\u001b[31mipython 6.5.0 has requirement prompt-toolkit<2.0.0,>=1.0.15, but you'll have prompt-toolkit 2.0.9 which is incompatible.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip -q install ./python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from collections import deque\n",
    "from unityagents import UnityEnvironment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the state and action spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"/data/Banana_Linux_NoVis/Banana.x86_64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up variables\n",
    "brain_name = \"BananaBrain\"\n",
    "state_size = 37\n",
    "action_size = 4\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Multilayer perceptron for action-value estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Multilayer perceptron with 3 hidden layers\n",
    "\n",
    "Params (init):\n",
    "1. input size\n",
    "2. hidden units in 1st, 2nd, 3rd hidden layers\n",
    "3. output size\n",
    "\n",
    "Forward propagation:\n",
    "- need to input state vector\n",
    "\n",
    "'''\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "\n",
    "#3 hidden layers\n",
    "class QNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_layer, hidden_1, hidden_2, hidden_3, output_layer):\n",
    "        \n",
    "        super(QNetwork,self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_layer,hidden_1)\n",
    "        self.fc2 = nn.Linear(hidden_1,hidden_2)\n",
    "        self.fc3 = nn.Linear(hidden_2,hidden_3)\n",
    "        self.fc4 = nn.Linear(hidden_3,output_layer)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        \n",
    "        return self.fc4(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Replay Buffer for Experience Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Replay buffer that stores the followings for experience replay:\n",
    "- current state\n",
    "- action (needed as actions are chosen by epsilon-greedy policy)\n",
    "- reward \n",
    "- next state\n",
    "- dones (for terminal states)\n",
    "\n",
    "Functions of replay buffer:\n",
    "1. contains self.memory (local) to store experiences as a double-ended queue\n",
    "2. add function to add new experience to buffer\n",
    "3. sample function that returns 64 samples for parallel training\n",
    "4. len function that returns length of replay buffer\n",
    "\n",
    "'''\n",
    "\n",
    "BUFFER_SIZE = 100000\n",
    "SAMPLE_SIZE = 64\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.memory = deque(maxlen=BUFFER_SIZE) \n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self):\n",
    "        \n",
    "        experiences = random.sample(self.memory, k=SAMPLE_SIZE)\n",
    "        \n",
    "        # Separates states, actions, rewards etc from sampled experience and vstack them for batch learning\n",
    "        states = torch.from_numpy(np.vstack([e[0] for e in experiences if e is not None])).float().cuda()\n",
    "        actions = torch.from_numpy(np.vstack([e[1] for e in experiences if e is not None])).long().cuda()\n",
    "        rewards = torch.from_numpy(np.vstack([e[2] for e in experiences if e is not None])).float().cuda()\n",
    "        next_states = torch.from_numpy(np.vstack([e[3] for e in experiences if e is not None])).float().cuda()\n",
    "        dones = torch.from_numpy(np.vstack([e[4] for e in experiences if e is not None]).astype(np.uint8)).float().cuda() \n",
    "        \n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Agent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Agent \n",
    "\n",
    "Params:\n",
    "1. QNetwork local - estimates action values for current state\n",
    "2. QNetwork target - estimates action values for the next state, finds the maximum for the generation of target values\n",
    "\n",
    "Functions:\n",
    "1. Act: forward propagation and generate actions based on epsilon-greedy policy\n",
    "2. Step: - stores experience into the memory of ReplayBuffer and learns when the number of samples in ReplayBuffer is > 64\n",
    "         - updates target network (occurs together with the learning step) with local network by TAU amount\n",
    "\n",
    "'''\n",
    "\n",
    "import torch.optim as optim\n",
    "import random\n",
    "\n",
    "GAMMA = 0.99\n",
    "learning_rate = 0.0005\n",
    "TAU = 0.001\n",
    "\n",
    "class Agent():\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.qnetwork_local = QNetwork(input_layer=state_size, hidden_1=64, hidden_2=128, hidden_3=64, output_layer=action_size).cuda()\n",
    "        self.qnetwork_target = QNetwork(input_layer=state_size,hidden_1=64,hidden_2=128,hidden_3=64,output_layer=action_size).cuda()\n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=learning_rate) #no need to optimize target network as it will be copies later\n",
    "        \n",
    "        self.memory = ReplayBuffer() \n",
    "        self.timestep = 0\n",
    "        \n",
    "    def act(self, state, eps):\n",
    "        \n",
    "        eps = eps\n",
    "        #unsqueeze as neural network expects dimension of [batch_size, channels, height, width]\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).cuda() \n",
    "        \n",
    "        self.qnetwork_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.qnetwork_local(state)\n",
    "        self.qnetwork_local.train()\n",
    "\n",
    "        if random.random() > eps:\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(4))\n",
    "    \n",
    "    def step(self, state, action, reward, next_action, done):\n",
    "        \n",
    "        # 1. add current experience into memory\n",
    "        self.memory.add(state, action, reward, next_action, done)\n",
    "        \n",
    "        # 2. learning every 5 steps, only if memory has > 64 tuples\n",
    "        self.timestep += 1\n",
    "        if self.timestep % 5 == 0:\n",
    "            if len(self.memory) > 64:\n",
    "                \n",
    "                experiences = self.memory.sample()\n",
    "                states, actions, rewards, next_states, dones = experiences\n",
    "                \n",
    "                # Get Q-targets with next_state and QNetwork target\n",
    "                Q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "                Q_targets = rewards + (GAMMA * Q_targets_next * (1 - dones))\n",
    "\n",
    "                # Get expected Q values from QNetwork local \n",
    "                Q_expected = self.qnetwork_local(states).gather(1, actions)\n",
    "\n",
    "                loss = F.mse_loss(Q_expected, Q_targets)\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                #Update target network by TAU amount\n",
    "                for target_param, local_param in zip(self.qnetwork_target.parameters(), self.qnetwork_local.parameters()):\n",
    "                    target_param.data.copy_(TAU*local_param.data + (1-TAU)*target_param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Learning through interacting with Unity environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint: Episode= 100 Average score= 0.256581\n",
      "Checkpoint: Episode= 200 Average score= 3.4655\n",
      "Checkpoint: Episode= 300 Average score= 7.3335\n",
      "Checkpoint: Episode= 400 Average score= 9.2735\n",
      "Checkpoint: Episode= 500 Average score= 12.445\n",
      "Checkpoint: Episode= 600 Average score= 13.566\n",
      "Checkpoint: Episode= 700 Average score= 15.141\n",
      "Checkpoint: Episode= 800 Average score= 15.017\n",
      "Checkpoint: Episode= 900 Average score= 15.414\n",
      "Episode: 999 Average Score= 16.131313131313132"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Checkpoint file:\n",
    "- checkpoint.pth that saves current parameters\n",
    "\n",
    "Note: need to specify the brain when using env.reset and env.step\n",
    "\n",
    "'''\n",
    "\n",
    "import math\n",
    "\n",
    "agent = Agent()\n",
    "maximum_score = -math.inf\n",
    "\n",
    "EPS_START = 1\n",
    "EPS_DECAY = 0.995\n",
    "EPS_MIN = 0.01\n",
    "\n",
    "total_for_average = 0\n",
    "run = 0\n",
    "\n",
    "eps = EPS_START\n",
    "\n",
    "for i_episode in range(1, 1000):\n",
    "    \n",
    "    env_info = env.reset(train_mode=True)[brain_name]     \n",
    "    state = env_info.vector_observations[0]\n",
    "    \n",
    "    score = 0    \n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        action = agent.act(state, eps)\n",
    "        \n",
    "        env_info = env.step(action)[brain_name]\n",
    "        next_state = env_info.vector_observations[0]\n",
    "        reward = env_info.rewards[0]                   \n",
    "        done = env_info.local_done[0]   \n",
    "        \n",
    "        agent.step(state, action, reward, next_state, done)\n",
    "        \n",
    "        state = next_state\n",
    "        \n",
    "        score += reward\n",
    "        total_for_average += reward\n",
    "        \n",
    "        if done:\n",
    "            \n",
    "            # print running average and saves parameters\n",
    "            run += 1\n",
    "            average = total_for_average/run\n",
    "            print(\"\\rEpisode: %s Average Score= %s\" % (i_episode,average), end=\"\")\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "            \n",
    "            # print average score every 100 episode\n",
    "            if i_episode % 100== 0:\n",
    "                print(\"\\rCheckpoint: Episode= %s Average score= %s\" % (i_episode,average))\n",
    "                total_for_average = 0\n",
    "                run = 0\n",
    "            \n",
    "            break\n",
    "            \n",
    "    eps = max(EPS_MIN,eps*EPS_DECAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
